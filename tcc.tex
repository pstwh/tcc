\documentclass[12pt]{article}

\usepackage{templates/sbc-template}

\usepackage{graphicx,url}
\usepackage{color}

\usepackage[brazil]{babel}
\usepackage[utf8]{inputenc}


\sloppy

\title{Identificação e classificação de comentários tóxicos utilizando processamento de linguagens naturais e técnicas de aprendizagem profunda}

\author{
    Miguel Angelo Cece de Castro Neto\inst{1},
    Paulo Alves dos Santos Junior\inst{1}\\
    Alceu de Souza Britto Junior\inst{2}
}


\address{
    Departamento de Informática -- Universidade Estadual de Ponta Grossa (UEPG)\\
    Avenida General Carlos Cavalcanti, 4748\\
    CEP 84030-900 – Ponta Grossa, PR – Brasil
    \nextinstitute
    Programa de Pós-Graduação em Informática (PPGIa) – Pontifícia Universidade\\
    Católica do Paraná (PUCPR)\\
    Rua Imaculada Conceição, 1155\\
    CEP 80215-901 – Curitiba, PR – Brasil
    \email{miguelceccineto@gmail.com, contato@pauloalvesjr.com,
     alceu@ppgia.pucpr.br}
}

\begin{document}

\maketitle

\begin{abstract}
  The article addresses the problem of multiclass sentiment analysis at the sentence level. Recurrent neural networks and their demonstrations have demonstrated successful modeling of sentiment classifiers and last generation in language modeling with the same demonstration demonstrating efficiency and performance in various tasks. The datasheet of the learned paper transfer, larger database, has proven effective in small databases. This work proposes the use ages between architecture in a with modeling of language to a classification of the sentiment in comment.
\end{abstract}

\begin{resumo}
  Este artigo aborda o problema da análise de sentimentos multiclasse no nível de sentença. Redes neurais recorrentes e suas variações demonstraram sucesso na modelagem de classificadores de sentimentos e recentemente a modelagem de linguagens através dessa arquitetura demonstrou-se eficaz atingindo o estado da arte em várias tarefas. O uso da técnica de transferência de aprendizado, proveniente de banco de dados maiores, se demonstrou eficiente em banco de dados pequenos. Nesse trabalho propomos o uso destas variações arquitetura em conjunto com a modelagem de linguagem para a classificação de sentimento em comentários.
\end{resumo}


\section{Introdução} \label{sec:introducao}

Discutir assuntos na internet pode ser difícil. Ameaças, ofensas pessoais, e assédio, podem criar um ambiente tóxico dentro de fóruns e redes sociais, e até afastar usuários. As plataformas lutam para combater efetivamente discussões tóxicas e limitar ou desligar usuários com essas práticas, porém essa é uma tarefa de difícil automação exigindo milhares de sentenças rotuladas e métodos capazes de analisar de maneira eficaz o contexto e significado de sentenças.

Recentemente, em resposta ao crescimento do uso de redes sociais, comentários são publicados massivamente. Como a escrita compõe grande parte de todos os dados gerados pela humanidade, que até então, apenas serviam para consultas, hoje servem como base de dados para o desenvolvimento de sistemas de processamento de linguagens naturais. Aplicando técnicas de aprendizagem profunda e processamento de linguagens naturais podemos automatizar processos análise e classificação de textos com uma boa taxa de acerto, comparado à classificação por métodos bayesianos, por exemplo.

No problema a ser discutido nesse artigo, com base de dados referente a comentários tóxicos, temos basicamente a análise de sentimentos conforme as classes rotuladas no mesmo, sendo utilizada para representar em quais classes de toxicidade determinada sentença pertence. Formalmente, o objetivo da análise de sentimentos nesse trabalho é extrair a seguinte sêxtupla:

\[
(a\lambda1, a\lambda2, a\lambda3, a\lambda4, a\lambda5, a\lambda6)
\]

Onde $a \lambda i$ se refere a probabilidade de cada classe, que respectivamente representam: Tóxico; Muito Tóxico; Obsceno; Ameaça; Insulto; Ódio de Identidade.

A análise de sentimentos é determinada comumente de maneira binária (positiva e negativa), porém abordagens multiclasse também são possíveis. Em uma abordagem multiclasse determinado comentário pode possuir diversos rótulos simultaneamente. Para definir a ativação de determinada classe, é definido um limiar. Tal limiar só é válido devido a função de ativação utilizada como saída na rede.

A utilização de redes neurais artificiais voltou a popularizar-se no reconhecimento de padrões em imagens devido a evolução de hardware, tais métodos de aprendizagem de máquina combinados ao processamento de linguagens naturais, permitem a avaliação automática de padrões e extração de conhecimento de bases de texto. A identificação de comentários ofensivos em textos é um derivado do caso geral de análise de sentimentos, na era da internet, identificar e classificar sentenças permite tomada de decisões e maior controle e filtragem de conteúdo. 

Esse trabalho tem como principal interesse, o estudo e experimento de técnicas de aprendizagem profunda (\textit{deep learning}) aplicada à área de processamento de linguagens naturais. Abordagens de aprendizagem de máquina tradicionais necessitam de intervenção humana para definição de características, existe a possibilidade de delegar tal tarefa à algoritmos simples de extração de características línguisticas ou sintáticas disponíveis na literatura, como utilizando contagem, ou processamento esparso. A tarefa realizada utilizando métodos clássicos, por fim utilizam algum método superficial de aprendizado, como por exemplo, máquinas de vetores de suporte \cite{DBLP:journals/ml/CortesV95}, naive bayes e máquinas de vetores de suport naive bayes  \cite{wang:2012}.

Embora os métodos descritos anteriormente já tenham sido validados, o foco desse trabalho é utilizar um método automátizado de extração de características que seja eficaz. Para a análise de sentimentos em comentários e classificação de toxicidade, utilizaremos utilizaremos redes neurais recorrentes em conjunto com estruturas celulares específicas, como células recorrentes bloqueadas (\textit{gated recurrent units}) \cite{DBLP:journals/corr/PascanuGCB13} e células de longa memória de curto termo (\textit{long-short term memory}) \cite{sep:97}. Abordaremos técnicas de transferência de aprendizado e construção de modelos de linguagem.
A estrutura recorrente é uma variação da rede neural clássica para aceitar entradas com tamanhos e saídas arbitrários, possuindo também registro de contexto em relação a posição do elemento em seu conjunto de entrada. Redes recorrentes comuns não são efetivas devido a problemas de esquecimento de aprendizado ou pouca capacidade de aprendizado devido a limitação imposta pela quantidade de parâmetros. Para resolver essa situação existem diferentes células que podem ser incorporadas nas arquiteturas, como por exemplo células recorrentes bloqueadas (\textit{gated recurrent units}) e células de longa memória de curto termo (\textit{long-short term memory}).

As redes neurais recorrentes possuem como característica a capacidade de trabalhar com entradas e saídas de tamanhos arbitrários, sendo uma ótima arquitetura para solução de problemas de múltiplas sentenças. É apenas necessário o pré-processamento de cada elemento, com o objetivo de remoção de ruídos e conversão para tensores numéricos \cite{karpathy:2015}.

A adição de convoluções, também podem ser utilizadas, para melhorar a detecção de padrões nas saídas de redes recorrentes. As redes convolucionais normalmente são utilizadas para extração de características em imagens \cite{lecun:98} ou quando as dimensões estão extremamente relacionadas. No caso imagens, são utilizadas convoluções de duas dimensões, no caso de vídeo ou objetos tridimensionais, três dimensões. Nesse artigo será avaliado o desempenho de convoluções de uma dimensão buscando o relacionamento dessas saidas.

Nesse trabalho as arquiteturas utilizadas aspiram utilizar os diversos conceitos disponíveis em busca de uma solução melhor que o aprendizado de máquina classico e conceitos individuais. Na seção 2 é descrito brevemente trabalhos relacionados na área, tanto em aprendizado de máquina clássico, quanto aprendizado de máquina profundo. Na seção 3 revisaremos os principais conceitos para o entendimento da seção 4, que se refere as arquiteturas utilizadas nesse artigo. Na seção 5 temos a análise e comparação de todos os modelos treinados, já na seção 6 nossas considerações finais e trabalhos futuros.

\section{Trabalho Relacionado} \label{sec:relacionado}

A análise de sentimentos está sob grande aprimoramento devido a aplicação de redes recorrentes \cite{karpathy:2015}, redes convolucionais \cite{lecun:98} e transferência de aprendizado \cite{DBLP:journals/corr/abs-1801-06146}. Esses casos descritos anteriormente se referem ao estado da arte dessa atividade.

Métodos clássicos de aprendizagem de máquina aplicados a análise e classificação de sentimento, como por exemplo \textit{Suport Vector Machines} \cite{DBLP:journals/ml/CortesV95}, apresentam resultados inferiores ao estado da arte. Essa abordagem necessida que as características sejam extraídas de maneira não automátizada, com ajuda de análise humanada, ou utilizando segmentações pouco eficientes, como por exemplo contagem de palavras e transformações esparsas.

O principal problema do método superficial citado acima, é que ele não é capaz de identificar o contexto de palavras, não sendo uma opção robusta para classificação.

Redes recorrentes e suas variações são aplicadas utilizando comumente matrizes que definem significado próximos ao real para cada palavra, como por exemplo \textit{Word2Vec} \cite{DBLP:journals/corr/abs-1301-3781} ou \textit{GloVe} \cite{pennington2014glove}. Tais métodos provem contexto utilizando palavras próximas para modelas matrizes e consequentemente contexto para redes recorrentes.

Transferência de aprendizado é uma técnica onde um modelo já treinado é reajustado para detectar padrões diferentes dos iniciais. Geralmente usado quando a base de dados disponível é menor do que a usada no modelo já treinado. Trabalhos recentes propõem a utilização de modelos de linguagem universais, que sofrerão afinação (fine tuning) para atender uma tarefa específica \cite{DBLP:journals/corr/abs-1801-06146} com a possibilidade de utilização de dados não supervisionados para gerar um aumento de performance (Radford et al. 2018). A utilização de dados não rotulados permite reduzir drasticamente a necessidade de exemplos rotulados. O estado da arte da maioria das tarefas na área de linguagens naturais está relacionado ao uso de transferência de aprendizado e aprendizado não supervisionado, que é um dos maiores desafios atualmente.

\section{Redes Neurais} \label{sec:revisao}

\subsection{Redes Neurais Recorrentes}

Redes recorrentes possuem o mesmo conceito básico de redes neurais artificiais, com o objetivo de otimizar parâmetros para retornar uma saída coerente em relação a entrada e ao problema. O que faz as redes recorrentes serem especiais, é o fato de elas não possuirem a restrição de tamanho fixo de entrada ou saída \cite{karpathy:2015}.

O conjunto de dados utilizados nesse artigo se adequa a arquitetura muitos para muitos, onde a entrada são as sequencias vetorizadas e tratadas através de \textit{word embeddings}) e a saída é um vetor de probabilidades das classes.

A imagem a seguir demonstra alguns casos de redes recorrentes possíveis:

\begin{figure}[ht]
\centering
\includegraphics[width=1\textwidth]{images/rnn-effectiveness-examples.jpeg}
\caption{
    Possibilidades de entrada de saída em redes reccorentes. \cite{karpathy:2015}
}
\label{fig:sigmoid}
\end{figure}

\subsection{Funções de ativação}

Funções de ativação não lineares, dão capacidades não lineares para redes neurais \cite{lecun:98}. As funções de ativação nos permitem ir além de problemas linearmente separáveis. Caso essas funções não sejam adicionadas, o modelo apenas terá a capacidade de resolver problemas simples, sendo que todas as camadas formarão apenas um modelo linear.

É possível visualizar de maneira trivial na representação abaixo:

\begin{equation}
    z_1(z_0; w, b) = z_0^T * w_1 + b
\end{equation}

\begin{equation}
    z_2(z_1; w, b) = z_1^T * w_2 + b
\end{equation}

\begin{equation}
    z_2(z_1; w, b) = (z_0^T * w_1 + b)^T * w_2 + b
\end{equation}

A função (3), mesmo após a progressão através das camadas, continua sendo um modelo linear.

\subsubsection{ReLU - \textit{Rectified Linear Activation Function}}

Dada pela fórmula:

\begin{equation}
    ReLU(Z) = max(0, Z)
\end{equation}

A imagem abaixo descreve expressão da ReLU em um intervalo de 5 a –5.

\begin{figure}[ht]
\centering
\includegraphics[width=.5\textwidth]{images/relu.png}
\caption{Unidade Linear Retificada}
\label{fig:relu}
\end{figure}

É a função de otimização padrão recomendada para uso na maioria das redes neurais. Aplicando essa função no output de uma transformação linear produz uma transformação não linear. A função permanece muito próxima de ser linear, contudo, no sentido de que é uma função linear por partes com duas peças lineares. Como as unidades lineares retificadas são quase lineares, elas preservam as propriedades que tornam os modelos lineares fáceis de otimizar com métodos baseados em gradiente. Eles também preservam muitas das propriedades que fazem modelos lineares generalizarem sistemas a partir de componentes mínimos \cite{Goodfellow-et-al-2016}.

\subsubsection{Sigmoide}

A função sigmoide é dada pela formula:

\begin{equation}
    \sigma(Z) = 1/(1+e^{-Z})
\end{equation}

A imagem abaixo descreve a curva sigmoidal em um intervalo de 5 a –5.

\begin{figure}[ht]
\centering
\includegraphics[width=.5\textwidth]{images/sigmoid.png}
\caption{Curva Sigmoidal}
\label{fig:sigmoid}
\end{figure}

Aplicando essa função no output de uma transformação linear, teremos como resultado um valor entre 0 e 1.

Além de a função sigmoide ser utilizada na estrutura de células de longa memória de curto termo (\textit{long-short term memory}) e células recorrentes bloqueadas (\textit{gated recurrent units}), também pode ser usada na saída do modelo. Poderíamos por exemplo, na classificação de sentimentos em texto, considerar valores próximos de 0 um sentimento negativo, e próximos de 1, positivo. Os valores maiores iguais a 0.5 serão classificados como positivos, enquanto 0.5 serão classificados como negativos.

\subsubsection{Tangente Hiperbólica}

Dada pela formula: 

\begin{equation}
    \sigma(Z) = 1/(1+e^{-Z})
\end{equation}

A imagem abaixo descreve a curva da tangente hiperbolica em um intervalo de 5 a -5.

\begin{figure}[ht]
\centering
\includegraphics[width=.5\textwidth]{images/tanh.png}
\caption{Curva Tangente Hiperbólica}
\label{fig:tanh}
\end{figure}

Produz um resultado entre 1 e -1, e é usada na estrutura de células simples de redes recorrentes, células de longa memória de curto termo (\textit{long-short term memory}) e células recorrentes bloqueadas (\textit{gated recurrent units}). Em comparação a função de ativação sigmoide, possui a vantagem de ser simétrica em relação ao eixo x.

\subsection{Otimizadores}

\subsubsection{Escolha do Algoritmo de Otimização}

Infelizmente ainda não existe um consenso sobre qual é o melhor algoritmo de otimização. Schaul et al. (2014) apresentou uma comparação valiosa com um grande número de algoritmos de otimização através de uma ampla variedade de tarefas de otimização. Enquanto os resultados sugeriam que a família de algoritmos com taxa de aprendizado adaptativa (como RMSProp e AdaDelta) apresentavam na maioria das tarefas desempenho superior aos outros métodos, porém, não houve um único algoritmo que emergiu sobre os outros.

Atualmente, os algoritmos de otimização mais utilizados na literatura são Gradiente Descendente Estocástico, Gradiente Descendente Estocástico com Momento, RMSProp, RMSProp com Momento, AdaDelta e Adam. A escolha de qual algoritmo de otimização usar, parece depender mais na familiaridade do desenvolvedor com o com o algoritmo (para afinar os hiperparametros mais facilmente) \cite{Goodfellow-et-al-2016}.

\subsubsection{Otimizador Adam}

Com a utilização do método minibatches, cujo divide a base de dados em varios blocos chamados minibatches, o progresso no gradiente tende a oscilar, logo o aprendizado pela rede se torna lento. Várias soluções foram propostas para solucionar esse problema, e a mais eficaz é uma junção das anteriores. O otimizador Adam \cite{DBLP:journals/corr/KingmaB14} utiliza a estimação do primeiro e do segundo momento dos gradientes, e terá a função de otimizar a função de custo.

\subsection{Função de Custo}

Um aspecto importante no desenvolvimento de redes neurais profundas é na escolha de função de custo. Felizmente, as funções de custo para redes neurais são mais ou menos as mesmas da de outros modelos paramétricos, assim como modelos lineares \cite{Goodfellow-et-al-2016}.

\subsubsection{Entropia Cruzada}

O cálculo do custo por entropia cruzada é dado pela seguinte formula:

\begin{equation}
    CE = -\sum\limits_{x} p(x)\log q(x)
\end{equation}

\subsection{Regularização}

Um problema central em aprendizado de máquina é como fazer um algoritmo que irá ter um bom desempenho não somente nos dados de treino, mas também em novos dados inseridos nele. Muitas estratégias usadas em aprendizado de máquina são explicitamente desenvolvidas para reduzir o erro no conjunto de teste, possivelmente à custa de um erro maior no conjunto de treino. Essas estratégias são conhecidas coletivamente como regularização \cite{Goodfellow-et-al-2016}.

O Dropout oferece pouco custo computacional, porém um método poderoso para regularização de uma ampla família de modelos (Srivastava et al. 2014).

Especificamente, dropout treina o conjunto consistindo através de sub-redes geradas pela remoção de unidades que fazem parte das camadas intermediarias ou de entrada como ilustra a figura abaixo: \cite{Goodfellow-et-al-2016}.

\begin{figure}[ht]
\centering
\includegraphics[width=.8\textwidth]{images/dropout.png}
\caption{Dropout}
\label{fig:dropout}
\end{figure}

\subsection{Inicialização de parâmetros}

Talvez a única propriedade conhecida com completa certeza é que parâmetros precisam "quebrar a simetria" entre as diferentes unidades. Se duas unidades internas com a mesma função de ativação, estão conectadas às mesmas entradas, então essas unidades devem ter diferentes parâmetros iniciais. Caso elas tenham os mesmos parâmetros iniciais então um algoritmo de aprendizado determinístico aplicado à um modelo e função de custo também determinísticos, irá conseguintemente atualizar essas duas unidades da mesma forma. Mesmo se o modelo ou o algoritmo de treino for capaz de usar aleatoriedade para computar diferentes atualizações para diferentes unidades (por exemplo, dropout), na maioria das vezes é melhor inicializar cada unidade para computar uma função diferente de todas as outras unidades. Isso pode ajudar a garantir que nenhum padrão de entrada seja perdido no espaço nulo da forward propagation e nenhum padrão de gradiente seja perdido no espaço nulo da back-propagation. O objetivo de ter cada unidade computando uma funçao diferente motiva a inicialização aleatória de parâmetros \cite{Goodfellow-et-al-2016}.

Comumente, estabelecemos constantes os valores dos biases escolhidos de maneira heurística, e inicializamos apenas os pesos aleatoriamente (Goodfellow et al. 2017).

\subsection{Word Embeddings}

Word embedding é o nome coletivo de um conjunto de técnicas de modelagem de linguagem e de aprendizado de recursos no processamento de linguagem natural, em que palavras ou frases do vocabulário são mapeadas para vetores de números reais. A figura 5 ilustra como funcionam esses vetores.
\begin{figure}
  \centering
  \includegraphics[width=1\textwidth]{images/wordembeddings.png}
  \caption{
    Representação de word embeddings. [Mikolov et al., NAACL HLT, 2013]
  }
  \label{}
\end{figure}

\subsubsection{Células recorrentes bloqueadas (GRU)}
células recorrentes bloqueadas (\textit{gated recurrent units}) são definidas pelas seguintes formulas:

\begin{equation}
    Ç<t> = tanh(Wc[\Gamma_r * c<t-1>, x<t>]+b_c)
\end{equation}

\begin{equation}
    \Gamma_u = \sigma(W_u[c<t-1>, x<t>]+b_u)
\end{equation}

\begin{equation}
    \Gamma_r = \sigma(W_r[c<t-1>, x<t>]+b_r)
\end{equation}

\begin{equation}
    c<t>=\Gamma_u*Ç<t>+(1-\Gamma_u)*c<t-1)
\end{equation}

\begin{equation}
    a<t>=c<t>
\end{equation}

\subsubsection{Longa memória de curto termo (LSTM)}

células de longa memória de curto termo (\textit{long-short term memory}) são definidas pelas seguintes formulas:

\begin{equation}
    Ç<t> = tanh(Wc[a<t-1>, x<t>]+b_c)
\end{equation}

\begin{equation}
    \Gamma_u = \sigma(W_u[c<t-1>, x<t>]+b_u)
\end{equation}

\begin{equation}
    \Gamma_f = \sigma(W_f[c<t-1>, x<t>]+b_f)
\end{equation}

\begin{equation}
    \Gamma_o = \sigma(W_o[c<t-1>, x<t>]+b_o)
\end{equation}

\begin{equation}
    c<t>=\Gamma_u*Ç<t>+(\Gamma_f)*c<t-1)
\end{equation}

\begin{equation}
    a<t>=\Gamma_o*c<t>
\end{equation}

\section{Arquiteturas Propostas}

Um modelo inicial foi definido rapidamente para obter o primeiro resultado, a seguir utilizamos a técnica de iteração. A técnica de iteração, consiste na ideia de definir uma arquitetura inicial, e definir hiperparâmetros rapidamente. Como ponto de partida, foi utilizado uma arquitetura bastante popular e extremamente efetiva, sendo sem transferência de aprendizado, o estado da arte.

\subsection{Rede recorrente com convolução}

\begin{figure}[ht]
\centering
\includegraphics[width=.5\textwidth]{images/graph.png}
\caption{Estrutura genérica de rede}
\label{fig:graph}
\end{figure}

\subsubsection{Entrada de dados}

\subsubsection{\textit{Word Embeddings}}

\subsubsection{\textit{Dropout}}

\subsubsection{Célula}

\subsubsection{Convolução}

\subsubsection{\texit{Pooling}}


Embedding se refere a essa codificação. Utilizamos o conceito de vetores globais para representação de palavras (\textit{GloVe}) pré-treinados em um crawler global.

Dropout previne o encaixe exagerado da função aos dados, logo faz o modelo generalizar melhor.

Célula define qual componente interno da rede neural recorrente será utilizado. Nessa arquitetura utilizamos variações entre 128 e 256 nós.

Convolução utiliza uma janela deslizante unidimensional com um filtro de tamanho três. Busca encontrar características e padrões.

Pooling diminuem tamanho da saída da convolução.

Concatenção junta os tensores de saída dos poolings.

Predição faz uma rede neural totalmente conectada e utiliza a função de ativação sigmoid, sendo que retornos maior que 0.5 serão considerados verdadeiros, e menores como falso.

Os hiperparâmetros do otimizar foram definidos através dos valores indicados no artigo do mesmo. A taxa de aprendizado utiliza do processo de iteração, e o valor utilizado nas análises preliminares é arbitrário, sendo inicialmente 1x10-3. Os pesos das células foram inicializador através da inicialização Glorot (Bengio & Glorot, 2010).

\subsection{Organização dos dados}

O banco de dados utilizou da técnica de separação de treino e teste, onde 90\% dos dados foi atribuída para treino e 10\% para validação. Não existe uma métrica fixa para a definição da quantidade a ser repartida, porém essa deve ser feita de acordo com a quantidade de dados rotulados. Como a validação segue apenas para verificar se não está acontecendo sobreajuste aos dados de treinamento, foi escolhido 10\% como dos hiperparametros. Essa técnica é conhecida como holdout. Em abordagens clássicas de aprendizado de máquina, normalmente é utilizada a validação cruzada, porém, em casos de aprendizagem profunda (\textit{deep learning}) isso se torna inviável devido a quantidade de processamento. Como modelos de aprendizagem profunda (\textit{deep learning}) possuem grande volume de dados, utilizaremos o holdout devido ao custo computacional.

A segmentação a seguir representa como os dados foram separados:

\begin{figure}[ht]
\centering
\includegraphics[width=1\textwidth]{images/datasplit.png}
\caption{Separacão de dados}
\label{fig:datasplit}
\end{figure}

A segmentação de testes não está inclusa nos dados públicos. Tal teste é feita através de dados não rotulados na plataforma Kaggle. É possível realizar um submissão com os rotulos e obter a acurácia. Essa segmentação não é de acesso público para evitar que modelos sobreajustem seus parâmetros, assim o resultado retornado garante a validade de generalização do modelo.

\section{Análises Preliminares}

Em primeira instância os modelos obtiveram resultados satisfatórios, alcançando acurácia próxima à melhor submissão deste problema.

\begin{table}[t]
  \small
  \centering
  \renewcommand{\arraystretch}{1.15}
  \begin{tabular}{llll}
    \hline
     Célula & Acurácia Treino & Acurácia Validação & Acurácia Teste \\
    \hline
    \multicolumn{4}{c}{Modelo monolítico} \\
    \hline
    GRU & 98,50\% & 98,33\% & 98,33\% \\
    LSTM & 98,50\% & 97,81\% & 98,26\% \\
    GRU Bidirecional & 98,55\% & 98,44\% & 98,30\% \\
    \hline
  \end{tabular}
  \caption{Resultados Preliminares}
  \label{tab:ptb}
\end{table}


Podemos observar que nesse conjunto de dados a bidirecionalidade não trás beneficios nestas instâncias. Outro fator de relevante é o desempenho do GRU ter sobresaido em relação a LSTM. O GRU é um modelo "simplificado" da LSTM tradicional.

Análises utilizando modelos de linguagem e transferência de aprendizado ainda estão sendo desenvolvidas.

\section{Considerações Finais e Trabalhos Futuros}

A comparação dos modelos testados demonstra como a combinação de modelos recorrentes e camadas convolucionais melhoram a acurácia. Técnicas como transferência de aprendizado em modelos de linguagem não foram aplicadas nesse trabalho, porém há ambição de aplica-los em trabalhos futuros. Há a possibilidade de reelaborar a célula que constrói a rede neural recorrente para atender melhor a este caso. Talvez a aplicação de uma rede de capsulas \cite{DBLP:journals/corr/abs-1710-09829} proporcionaria um resultado superior devido a não variação em translações, que é uma das características desta arquitetura. Existe a ambição de aplicar conceitos presentes nesse trabalho em outras base de dados, com ajuda de aprendizagem semi-supervisionada e transferência de aprendizado.

\bibliographystyle{templates/sbc}
\bibliography{bibliography}

\end{document}
